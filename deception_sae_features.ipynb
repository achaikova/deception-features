{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DnL867I1CyKw",
    "outputId": "51757298-bb46-4f6c-ee6d-d459e409218c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.0/920.0 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m116.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.0/238.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.7/196.7 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for py2store (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
      "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\n",
      "inflect 7.4.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\n",
      "notebook 6.5.5 requires pyzmq<25,>=17, but you have pyzmq 26.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  %pip install -q  sae-lens transformer-lens\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ByyObG4cC3nw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import numpy as np\n",
    "from typing import Dict, Union, List\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")\n",
    "from google.colab import drive\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jr_BzvvkC4Gg"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3KHl1pFAnsw"
   },
   "source": [
    "#### Load model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "F1Q3EfPfttue"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QW4iF61QC53Q"
   },
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "\n",
    "layer = 10\n",
    "\n",
    "# get the SAE for this layer\n",
    "sae, cfg_dict, _ = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-27b-pt-res-canonical\",\n",
    "    sae_id = f\"layer_{layer}/width_131k/canonical\",\n",
    "    device = device\n",
    ")\n",
    "hook_point = sae.cfg.hook_name\n",
    "print(hook_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iU5TLqYuhrR"
   },
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"google/gemma-2-27b-it\",\n",
    "                                          dtype='float16',\n",
    "                                          device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaMksTRoAqLD"
   },
   "source": [
    "#### Load prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMapiuYVA7p7"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "        drive.mount('/content/drive')\n",
    "        folder_path = '/content/drive/My Drive/data'\n",
    "except:\n",
    "    folder_path = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJL5lgaSVw87"
   },
   "outputs": [],
   "source": [
    "def load_json_files(folder_path):\n",
    "    json_data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            with open(os.path.join(folder_path, filename), 'r') as f:\n",
    "                json_data.append(json.load(f))\n",
    "    return json_data\n",
    "\n",
    "json_data = load_json_files(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4u2X782V0fy"
   },
   "outputs": [],
   "source": [
    "def extract_data(json_data):\n",
    "    result = {\"ans_always_a\": [], \"suggested_answer\": []}\n",
    "\n",
    "    pattern = re.compile(r'-biastype([a-zA-Z_]+)-')\n",
    "\n",
    "    for data in json_data:\n",
    "        match = pattern.search(data['filename'])\n",
    "        if match:\n",
    "            bias_type = match.group(1)\n",
    "\n",
    "            outputs_0 = data['outputs']['0']\n",
    "            outputs_1 = data['outputs']['1']\n",
    "\n",
    "            if bias_type == 'ans_always_a':\n",
    "                # Criteria 1: \"0\" y_pred == 0 and \"1\" y_pred == 1\n",
    "                for i in range(len(outputs_0['gen'])):\n",
    "                    if outputs_0['y_pred'][i] == 0 and outputs_1['y_pred'][i] == 1:\n",
    "                        result[\"ans_always_a\"].append({\n",
    "                            \"gen_0\": outputs_0['gen'][i],\n",
    "                            \"gen_1\": outputs_0['gen'][i],\n",
    "                            \"input_0\": outputs_0['inputs'][i],\n",
    "                            \"input_1\": outputs_1['inputs'][i]\n",
    "                        })\n",
    "\n",
    "            elif bias_type == 'suggested_answer':\n",
    "                # Criteria 2: \"0\" y_pred == random_ans_idx and \"1\" y_pred != random_ans_idx\n",
    "                for i in range(len(outputs_0['gen'])):\n",
    "                    random_ans_idx = outputs_0['random_ans_idx'][i]\n",
    "                    if outputs_0['y_pred'][i] == random_ans_idx and outputs_1['y_pred'][i] != random_ans_idx:\n",
    "                        result[\"suggested_answer\"].append({\n",
    "                            \"gen_0\": outputs_0['gen'][i],\n",
    "                            \"gen_1\": outputs_0['gen'][i],\n",
    "                            \"input_0\": outputs_0['inputs'][i],\n",
    "                            \"input_1\": outputs_1['inputs'][i]\n",
    "                        })\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SvdW1yHjWY6m"
   },
   "outputs": [],
   "source": [
    "extracted_data = extract_data(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aWVDVvaAtl6"
   },
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKEMK46I_9T-"
   },
   "outputs": [],
   "source": [
    "def get_features_per_prompt(prompt):\n",
    "    _, cache = model.run_with_cache_with_saes(prompt, saes=[sae])\n",
    "    vals, inds = torch.topk(cache[f\"blocks.{layer}.hook_resid_post.hook_sae_acts_post\"][0, -1, :], 5)\n",
    "    return vals, inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvTyHz6lws8r"
   },
   "outputs": [],
   "source": [
    "def get_features_per_generation(prompt, max_new_tokens):\n",
    "    features_fired = []\n",
    "    for i in range(max_new_tokens):\n",
    "        generation = model.generate(prompt, max_new_tokens=1)\n",
    "        vals, inds = get_features_per_prompt(prompt)\n",
    "        features_fired.append({\"features\": inds.tolist(), \"values\": vals.tolist()})\n",
    "        prompt = generation\n",
    "    return prompt, features_fired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1gZuuNXvDEv"
   },
   "outputs": [],
   "source": [
    "def self_explanation(feature, scale, max_new_tokens=40, n_generate=10):\n",
    "    prompt = '<start_of_turn>user\\nWhat is the meaning of the word \"X\"?<end_of_turn>\\n<start_of_turn>model\\nThe meaning of the word \"X\" is \"'\n",
    "    positions = [i for i, a in enumerate(model.tokenizer.encode(prompt)) if model.tokenizer.decode([a]) == \"X\"]\n",
    "    vector = sae.W_dec[[feature]]\n",
    "    vector = vector / vector.norm()\n",
    "    vector = vector * scale\n",
    "    def rep_hook(resid_pre, hook):\n",
    "        if resid_pre.shape[1] == 1:\n",
    "            return\n",
    "        for position in positions:\n",
    "            resid_pre[:, position] = vector\n",
    "\n",
    "    with model.hooks(fwd_hooks=[(f\"blocks.{layer}.hook_resid_pre\", rep_hook)]):\n",
    "        result = model.generate(\n",
    "            stop_at_eos=False,  # avoids a bug on MPS\n",
    "            input=model.to_tokens([prompt] * n_generate),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True)\n",
    "    for i, l in enumerate(model.to_string(result)):\n",
    "        print(f\"{i+1}.\", repr(l.partition(prompt)[2].partition(\"<eos>\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZoUi9TUIYg3D"
   },
   "outputs": [],
   "source": [
    "def compare_feature_activations(input_0, input_1, layer, top_k=5):\n",
    "    prompts = [input_0, input_1]\n",
    "    _, cache = model.run_with_cache_with_saes(prompts, saes=[sae])\n",
    "\n",
    "    activations_0 = cache[f\"blocks.{layer}.hook_resid_post.hook_sae_acts_post\"][0, -1, :].cpu()\n",
    "    activations_1 = cache[f\"blocks.{layer}.hook_resid_post.hook_sae_acts_post\"][1, -1, :].cpu()\n",
    "\n",
    "    diff = activations_1 - activations_0\n",
    "    vals, inds = torch.topk(torch.abs(diff), top_k)\n",
    "\n",
    "    feature_differences = []\n",
    "    for val, ind in zip(vals, inds):\n",
    "        feature_differences.append({\n",
    "            \"feature\": ind.item(),\n",
    "            \"value_0\": activations_0[ind].item(),\n",
    "            \"value_1\": activations_1[ind].item(),\n",
    "            \"difference\": val.item()\n",
    "        })\n",
    "\n",
    "    return feature_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bljx1lQZa2iT"
   },
   "outputs": [],
   "source": [
    "def track_feature_frequencies(prompt, max_new_tokens, num_repeats=5, verbose=True, top_k=None):\n",
    "    feature_frequencies = defaultdict(int)\n",
    "\n",
    "    for _ in range(num_repeats):\n",
    "        generation, features_fired_per_run = get_features_per_generation(prompt, max_new_tokens)\n",
    "        if verbose:\n",
    "            print(f\"Prompt: {prompt}\\nGeneration: {generation}\", end='\\n\\n---------\\n\\n')\n",
    "        for step_features in features_fired_per_run:\n",
    "            for feature in step_features[\"features\"]:\n",
    "                feature_frequencies[feature] += 1\n",
    "\n",
    "    sorted_feature_frequencies = sorted(feature_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if top_k:\n",
    "      most_frequent_features = sorted_feature_frequencies[:top_k]\n",
    "\n",
    "    return most_frequent_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJks2rXFbiK7"
   },
   "outputs": [],
   "source": [
    "def run_feature_tracking_for_generations(extracted_data, max_new_tokens, num_repeats=5, top_k=5):\n",
    "    all_feature_frequencies = {\"ans_always_a\": [], \"suggested_answer\": []}\n",
    "\n",
    "    for bias_type in extracted_data.keys():\n",
    "        for entry in extracted_data[bias_type]:\n",
    "            input_0 = entry['input_0']\n",
    "            input_1 = entry['input_1']\n",
    "\n",
    "            top_features_biased = track_feature_frequencies(input_0, max_new_tokens, num_repeats, top_k)\n",
    "            top_features_unbiased = track_feature_frequencies(input_1, max_new_tokens, num_repeats, top_k)\n",
    "\n",
    "            all_feature_frequencies[bias_type].append({\n",
    "                \"input_0\": input_0,\n",
    "                \"input_1\": input_1,\n",
    "                \"top_features_biased\": top_features_biased,\n",
    "                \"top_features_unbiased\": top_features_unbiased\n",
    "            })\n",
    "\n",
    "    return all_feature_frequencies"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
